import json
import os
import re
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Configuration
CLEANED_DATA_DIR = "cleaned_data"
OUTPUT_DIR = "DOCUMENTS"
OUTPUT_REPORT = "COT_THINKING_ANALYSIS.md"

# Keywords to extract
TECHNICAL_INDICATORS = [
    "rsi", "macd", "ema", "sma", "æ”¯æ’‘", "é˜»åŠ›", "support", "resistance",
    "bollinger", "atr", "volume", "æˆäº¤é‡"
]

MARKET_CONCEPTS = [
    "trend", "è¶‹åŠ¿", "breakout", "çªç ´", "pullback", "å›è°ƒ", "consolidation",
    "éœ‡è¡", "reversal", "åè½¬", "momentum", "åŠ¨é‡", "bullish", "çœ‹æ¶¨",
    "bearish", "çœ‹è·Œ", "rally", "ä¸Šæ¶¨", "decline", "ä¸‹è·Œ"
]

TIMEFRAMES = [
    "3-min", "3 min", "15-min", "15 min", "1-hour", "1 hour", "4-hour",
    "4 hour", "daily", "æ—¥çº¿", "intraday", "ç›˜ä¸­", "short-term", "çŸ­æœŸ",
    "long-term", "é•¿æœŸ"
]

RISK_WORDS = [
    "risk", "é£é™©", "stop loss", "æ­¢æŸ", "profit target", "æ­¢ç›ˆ",
    "invalidation", "å¤±æ•ˆ", "cautious", "è°¨æ…", "conservative", "ä¿å®ˆ",
    "aggressive", "æ¿€è¿›", "confident", "ä¿¡å¿ƒ"
]


def load_model_trades(model_id: str) -> List[Dict]:
    """Load trades for a specific model"""
    filepath = os.path.join(CLEANED_DATA_DIR, f"{model_id}_trades.json")
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {model_id}: {e}")
        return []


def convert_timestamp(timestamp: float) -> str:
    """Convert Unix timestamp to readable format"""
    return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')


def extract_keywords(text: str, keywords_list: List[str]) -> List[str]:
    """Extract keywords from text"""
    if not text or not isinstance(text, str):
        return []
    
    text_lower = text.lower()
    found = []
    
    for keyword in keywords_list:
        if keyword.lower() in text_lower:
            found.append(keyword)
    
    return found


def analyze_cot_text(cot_trace: Any) -> Dict:
    """Analyze COT trace text"""
    # Handle different formats of cot_trace
    if isinstance(cot_trace, dict):
        # If it's a dict, try to get text content
        cot_text = str(cot_trace)
    elif isinstance(cot_trace, str):
        cot_text = cot_trace
    else:
        cot_text = str(cot_trace)
    
    analysis = {
        "length": len(cot_text),
        "technical_indicators": extract_keywords(cot_text, TECHNICAL_INDICATORS),
        "market_concepts": extract_keywords(cot_text, MARKET_CONCEPTS),
        "timeframes": extract_keywords(cot_text, TIMEFRAMES),
        "risk_words": extract_keywords(cot_text, RISK_WORDS),
        "text": cot_text
    }
    
    return analysis


def analyze_model_thinking(model_id: str, trades: List[Dict]) -> Dict:
    """Analyze thinking patterns for a model"""
    print(f"\nAnalyzing COT for {model_id}...")
    
    all_keywords = {
        "technical_indicators": [],
        "market_concepts": [],
        "timeframes": [],
        "risk_words": []
    }
    
    thinking_lengths = []
    
    trades_with_analysis = []
    
    for trade in trades:
        cot_trace = trade.get("cot_trace", "")
        cot_analysis = analyze_cot_text(cot_trace)
        
        # Collect keywords
        all_keywords["technical_indicators"].extend(cot_analysis["technical_indicators"])
        all_keywords["market_concepts"].extend(cot_analysis["market_concepts"])
        all_keywords["timeframes"].extend(cot_analysis["timeframes"])
        all_keywords["risk_words"].extend(cot_analysis["risk_words"])
        
        thinking_lengths.append(cot_analysis["length"])
        
        # Add analysis to trade
        trade_with_analysis = {
            **trade,
            "cot_analysis": cot_analysis
        }
        trades_with_analysis.append(trade_with_analysis)
    
    # Count frequencies
    keyword_counts = {
        "technical_indicators": Counter(all_keywords["technical_indicators"]),
        "market_concepts": Counter(all_keywords["market_concepts"]),
        "timeframes": Counter(all_keywords["timeframes"]),
        "risk_words": Counter(all_keywords["risk_words"])
    }
    
    avg_length = sum(thinking_lengths) / len(thinking_lengths) if thinking_lengths else 0
    
    print(f"  Average COT length: {avg_length:.0f} characters")
    print(f"  Total trades analyzed: {len(trades)}")
    
    return {
        "model_id": model_id,
        "total_trades": len(trades),
        "avg_thinking_length": avg_length,
        "keyword_counts": keyword_counts,
        "trades": trades_with_analysis
    }


def find_best_and_worst_cases(trades: List[Dict], top_n: int = 5) -> Tuple[List, List]:
    """Find best and worst performing trades"""
    # Calculate PnL for each trade
    for trade in trades:
        prev_value = trade.get("prev_account_value", 0)
        curr_value = trade.get("curr_account_value", 0)
        trade["pnl"] = curr_value - prev_value
        trade["pnl_pct"] = (trade["pnl"] / prev_value * 100) if prev_value > 0 else 0
    
    # Sort by PnL
    sorted_trades = sorted(trades, key=lambda x: x["pnl"], reverse=True)
    
    best_trades = sorted_trades[:top_n]
    worst_trades = sorted_trades[-top_n:]
    
    return best_trades, worst_trades


def generate_trade_case_markdown(trade: Dict, case_number: int, case_type: str) -> str:
    """Generate markdown for a single trade case with full original data"""
    md = f"### æ¡ˆä¾‹ {case_number}: {trade['model_id']} - "
    
    # Determine trade type
    changes = trade.get("position_changes", [])
    if changes:
        change_types = [c["change_type"] for c in changes]
        md += f"{', '.join(change_types)}\n\n"
    else:
        md += "äº¤æ˜“\n\n"
    
    # Time and cycle info
    timestamp = trade.get("timestamp", 0)
    time_str = convert_timestamp(timestamp)
    cycle_id = trade.get("cycle_id", "N/A")
    
    md += f"ğŸ“… **æ—¶é—´**: {time_str} (timestamp: {timestamp})\n"
    md += f"ğŸ”¢ **Cycle ID**: {cycle_id}\n"
    
    # Trade result
    pnl = trade.get("pnl", 0)
    pnl_pct = trade.get("pnl_pct", 0)
    emoji = "ğŸ’°" if pnl > 0 else "ğŸ’¸"
    md += f"{emoji} **äº¤æ˜“ç»“æœ**: ${pnl:+.2f} ({pnl_pct:+.2f}%)\n"
    
    # Account change
    prev_value = trade.get("prev_account_value", 0)
    curr_value = trade.get("curr_account_value", 0)
    md += f"ğŸ“Š **è´¦æˆ·å˜åŒ–**: ${prev_value:.2f} â†’ ${curr_value:.2f}\n"
    
    # Position changes
    if changes:
        md += f"\n**æŒä»“å˜åŒ–**:\n"
        for change in changes:
            symbol = change.get("symbol", "N/A")
            prev_qty = change.get("prev_quantity", 0)
            curr_qty = change.get("curr_quantity", 0)
            change_type = change.get("change_type", "N/A")
            md += f"- {symbol}: {prev_qty:.2f} â†’ {curr_qty:.2f} ({change_type})\n"
    
    md += "\n"
    
    # Full COT trace in expandable section
    cot_trace = trade.get("cot_trace", "")
    cot_summary = trade.get("cot_trace_summary", "")
    
    # Handle dict format
    if isinstance(cot_trace, dict):
        cot_trace_text = json.dumps(cot_trace, indent=2, ensure_ascii=False)
    else:
        cot_trace_text = str(cot_trace)
    
    md += f"ğŸ’­ **å®Œæ•´æ€è€ƒè¿‡ç¨‹** (ç‚¹å‡»å±•å¼€):\n"
    md += f"<details>\n"
    md += f"<summary>æŸ¥çœ‹å®Œæ•´COTåŸæ–‡ ({len(cot_trace_text)} å­—ç¬¦)</summary>\n\n"
    md += f"```\n{cot_trace_text}\n```\n\n"
    md += f"</details>\n\n"
    
    # Summary
    if cot_summary:
        md += f"ğŸ“ **æ€è€ƒæ‘˜è¦**:\n"
        md += f"> {cot_summary}\n\n"
    
    # Key findings based on case type
    cot_analysis = trade.get("cot_analysis", {})
    
    if case_type == "success":
        md += f"ğŸ¯ **æˆåŠŸè¦ç´ **:\n"
    else:
        md += f"âš ï¸ **é—®é¢˜åˆ†æ**:\n"
    
    # Analyze keywords
    tech_indicators = cot_analysis.get("technical_indicators", [])
    if tech_indicators:
        md += f"- ä½¿ç”¨çš„æŠ€æœ¯æŒ‡æ ‡: {', '.join(set(tech_indicators))}\n"
    
    timeframes = cot_analysis.get("timeframes", [])
    if timeframes:
        md += f"- å…³æ³¨çš„æ—¶é—´å‘¨æœŸ: {', '.join(set(timeframes))}\n"
    
    risk_words = cot_analysis.get("risk_words", [])
    if risk_words:
        md += f"- é£é™©ç®¡ç†å…³é”®è¯: {', '.join(set(risk_words))}\n"
    
    md += f"- æ€è€ƒå¤æ‚åº¦: {cot_analysis.get('length', 0)} å­—ç¬¦\n"
    
    md += "\n---\n\n"
    
    return md


def generate_markdown_report(all_analysis: Dict[str, Dict]):
    """Generate comprehensive COT analysis report"""
    print("\nGenerating COT analysis report...")
    
    report_path = os.path.join(OUTPUT_DIR, OUTPUT_REPORT)
    
    with open(report_path, "w", encoding="utf-8") as f:
        # Header
        f.write("# COTæ€è€ƒåˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("**åˆ†æèŒƒå›´**: 561ä¸ªäº¤æ˜“çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹\n\n")
        f.write("**æŠ¥å‘Šç‰¹ç‚¹**: æ¯ä¸ªæ´è§éƒ½é™„å¸¦å®Œæ•´åŸå§‹æ•°æ®å¼•ç”¨\n\n")
        f.write("---\n\n")
        
        # Table of contents
        f.write("## ç›®å½•\n\n")
        f.write("1. [å„æ¨¡å‹æ€è€ƒé£æ ¼æ¦‚è§ˆ](#å„æ¨¡å‹æ€è€ƒé£æ ¼æ¦‚è§ˆ)\n")
        f.write("2. [æˆåŠŸæ¨¡å‹çš„æ€è€ƒæ¨¡å¼](#æˆåŠŸæ¨¡å‹çš„æ€è€ƒæ¨¡å¼)\n")
        f.write("3. [å¤±è´¥æ¨¡å‹çš„æ€è€ƒé™·é˜±](#å¤±è´¥æ¨¡å‹çš„æ€è€ƒé™·é˜±)\n")
        f.write("4. [æœ€ä½³äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ](#æœ€ä½³äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ)\n")
        f.write("5. [æœ€å·®äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ](#æœ€å·®äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ)\n")
        f.write("6. [å…³é”®æ´å¯Ÿæ€»ç»“](#å…³é”®æ´å¯Ÿæ€»ç»“)\n\n")
        f.write("---\n\n")
        
        # Section 1: Overview
        f.write("## å„æ¨¡å‹æ€è€ƒé£æ ¼æ¦‚è§ˆ\n\n")
        
        # Create comparison table
        f.write("### æ€è€ƒç‰¹å¾å¯¹æ¯”\n\n")
        f.write("| æ¨¡å‹ | å¹³å‡æ€è€ƒé•¿åº¦ | ä¸»è¦æŠ€æœ¯æŒ‡æ ‡ | å…³æ³¨æ—¶é—´å‘¨æœŸ | é£é™©ç®¡ç†è¯æ±‡ |\n")
        f.write("|------|-------------|-------------|-------------|-------------|\n")
        
        for model_id, analysis in sorted(all_analysis.items()):
            avg_len = analysis["avg_thinking_length"]
            
            # Top technical indicators
            tech_counts = analysis["keyword_counts"]["technical_indicators"]
            top_tech = [k for k, v in tech_counts.most_common(3)]
            tech_str = ", ".join(top_tech) if top_tech else "N/A"
            
            # Top timeframes
            time_counts = analysis["keyword_counts"]["timeframes"]
            top_time = [k for k, v in time_counts.most_common(2)]
            time_str = ", ".join(top_time) if top_time else "N/A"
            
            # Risk words count
            risk_count = sum(analysis["keyword_counts"]["risk_words"].values())
            
            f.write(f"| {model_id} | {avg_len:.0f} | {tech_str} | {time_str} | {risk_count} |\n")
        
        f.write("\n")
        
        # Detailed keyword analysis
        f.write("### å…³é”®è¯é¢‘ç‡ç»Ÿè®¡\n\n")
        
        for model_id, analysis in sorted(all_analysis.items()):
            f.write(f"#### {model_id}\n\n")
            
            # Technical indicators
            tech_counts = analysis["keyword_counts"]["technical_indicators"]
            if tech_counts:
                f.write("**æŠ€æœ¯æŒ‡æ ‡ä½¿ç”¨é¢‘ç‡** (Top 10):\n")
                for keyword, count in tech_counts.most_common(10):
                    f.write(f"- {keyword}: {count}æ¬¡\n")
                f.write("\n")
            
            # Market concepts
            market_counts = analysis["keyword_counts"]["market_concepts"]
            if market_counts:
                f.write("**å¸‚åœºæ¦‚å¿µæåŠé¢‘ç‡** (Top 10):\n")
                for keyword, count in market_counts.most_common(10):
                    f.write(f"- {keyword}: {count}æ¬¡\n")
                f.write("\n")
            
            # Timeframes
            time_counts = analysis["keyword_counts"]["timeframes"]
            if time_counts:
                f.write("**æ—¶é—´å‘¨æœŸå…³æ³¨**:\n")
                for keyword, count in time_counts.most_common():
                    f.write(f"- {keyword}: {count}æ¬¡\n")
                f.write("\n")
            
            f.write("---\n\n")
        
        # Section 2: Success patterns
        f.write("## æˆåŠŸæ¨¡å‹çš„æ€è€ƒæ¨¡å¼\n\n")
        
        success_models = ["qwen3-max", "deepseek-chat-v3.1"]
        
        for model_id in success_models:
            if model_id in all_analysis:
                analysis = all_analysis[model_id]
                f.write(f"### {model_id}\n\n")
                
                f.write("**æ€è€ƒç‰¹ç‚¹**:\n")
                f.write(f"- å¹³å‡æ€è€ƒé•¿åº¦: {analysis['avg_thinking_length']:.0f} å­—ç¬¦\n")
                
                tech_counts = analysis["keyword_counts"]["technical_indicators"]
                f.write(f"- æŠ€æœ¯æŒ‡æ ‡ä½¿ç”¨: {len(tech_counts)} ç§ï¼Œæ€»è®¡ {sum(tech_counts.values())} æ¬¡æåŠ\n")
                
                risk_counts = analysis["keyword_counts"]["risk_words"]
                f.write(f"- é£é™©ç®¡ç†è¯æ±‡: {sum(risk_counts.values())} æ¬¡æåŠ\n\n")
                
                f.write("---\n\n")
        
        # Section 3: Failure patterns
        f.write("## å¤±è´¥æ¨¡å‹çš„æ€è€ƒé™·é˜±\n\n")
        
        failure_models = ["gemini-2.5-pro", "gpt-5"]
        
        for model_id in failure_models:
            if model_id in all_analysis:
                analysis = all_analysis[model_id]
                f.write(f"### {model_id}\n\n")
                
                f.write("**æ€è€ƒç‰¹ç‚¹**:\n")
                f.write(f"- å¹³å‡æ€è€ƒé•¿åº¦: {analysis['avg_thinking_length']:.0f} å­—ç¬¦\n")
                f.write(f"- æ€»äº¤æ˜“æ¬¡æ•°: {analysis['total_trades']} (è¿‡é«˜)\n")
                
                tech_counts = analysis["keyword_counts"]["technical_indicators"]
                f.write(f"- æŠ€æœ¯æŒ‡æ ‡ä½¿ç”¨: {len(tech_counts)} ç§ï¼Œæ€»è®¡ {sum(tech_counts.values())} æ¬¡æåŠ\n\n")
                
                f.write("---\n\n")
        
        # Section 4: Best trade cases
        f.write("## æœ€ä½³äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ\n\n")
        f.write("ä»¥ä¸‹å±•ç¤ºæ¯ä¸ªæ¨¡å‹çš„æœ€ä½³äº¤æ˜“ï¼ŒåŒ…å«å®Œæ•´åŸå§‹æ•°æ®ä¾›å‚è€ƒã€‚\n\n")
        
        case_number = 1
        for model_id, analysis in sorted(all_analysis.items()):
            f.write(f"### {model_id} æœ€ä½³äº¤æ˜“\n\n")
            
            best_trades, _ = find_best_and_worst_cases(analysis["trades"], top_n=3)
            
            for trade in best_trades:
                case_md = generate_trade_case_markdown(trade, case_number, "success")
                f.write(case_md)
                case_number += 1
        
        # Section 5: Worst trade cases
        f.write("## æœ€å·®äº¤æ˜“æ¡ˆä¾‹æ·±åº¦å‰–æ\n\n")
        f.write("ä»¥ä¸‹å±•ç¤ºæ¯ä¸ªæ¨¡å‹çš„æœ€å·®äº¤æ˜“ï¼Œåˆ†æå¤±è´¥åŸå› ã€‚\n\n")
        
        case_number = 1
        for model_id, analysis in sorted(all_analysis.items()):
            f.write(f"### {model_id} æœ€å·®äº¤æ˜“\n\n")
            
            _, worst_trades = find_best_and_worst_cases(analysis["trades"], top_n=3)
            
            for trade in worst_trades:
                case_md = generate_trade_case_markdown(trade, case_number, "failure")
                f.write(case_md)
                case_number += 1
        
        # Section 6: Key insights
        f.write("## å…³é”®æ´å¯Ÿæ€»ç»“\n\n")
        
        f.write("### æˆåŠŸæ€è€ƒæ¨¡å¼çš„å…±åŒç‰¹å¾\n\n")
        f.write("1. **é€‚åº¦çš„æ€è€ƒå¤æ‚åº¦**: æ—¢ä¸è¿‡åº¦ç®€å•ä¹Ÿä¸è¿‡åº¦å¤æ‚\n")
        f.write("2. **æ˜ç¡®çš„é£é™©ç®¡ç†**: é¢‘ç¹æåŠæ­¢æŸã€é£é™©æ§åˆ¶\n")
        f.write("3. **å¤šç»´åº¦åˆ†æ**: ç»“åˆæŠ€æœ¯æŒ‡æ ‡å’Œå¸‚åœºæ¦‚å¿µ\n")
        f.write("4. **é•¿æœŸè§†è§’**: å…³æ³¨è¾ƒé•¿æ—¶é—´å‘¨æœŸ\n\n")
        
        f.write("### å¤±è´¥æ€è€ƒæ¨¡å¼çš„å…±åŒé™·é˜±\n\n")
        f.write("1. **è¿‡åº¦ä¾èµ–å•ä¸€æŒ‡æ ‡**: ç‰¹åˆ«æ˜¯çŸ­æœŸæŠ€æœ¯æŒ‡æ ‡\n")
        f.write("2. **ç¼ºä¹é£é™©æ„è¯†**: å¾ˆå°‘æåŠæ­¢æŸå’Œé£é™©æ§åˆ¶\n")
        f.write("3. **çŸ­æœŸè§†è§’ä¸»å¯¼**: è¿‡åº¦å…³æ³¨çŸ­æœŸæ³¢åŠ¨\n")
        f.write("4. **å†³ç­–è¿‡äºé¢‘ç¹**: å¯¼è‡´æ€è€ƒè´¨é‡ä¸‹é™\n\n")
        
        f.write("---\n\n")
        f.write("**æŠ¥å‘Šè¯´æ˜**: æ‰€æœ‰æ¡ˆä¾‹éƒ½åŒ…å«å®Œæ•´çš„åŸå§‹æ•°æ®ï¼ˆtimestamp, cycle_id, COTåŸæ–‡ï¼‰ï¼Œ")
        f.write("å¯ç”¨äºè¿›ä¸€æ­¥éªŒè¯å’Œæ·±å…¥åˆ†æã€‚\n\n")
        f.write(f"**æ•°æ®æ¥æº**: cleaned_data/ (561ä¸ªäº¤æ˜“è®°å½•)\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    print(f"Report saved: {report_path}")


def main():
    """Main execution"""
    print("=" * 60)
    print("COT THINKING ANALYSIS")
    print("=" * 60)
    
    all_models = [
        "qwen3-max",
        "deepseek-chat-v3.1",
        "claude-sonnet-4-5",
        "grok-4",
        "gemini-2.5-pro",
        "gpt-5"
    ]
    
    all_analysis = {}
    
    for model_id in all_models:
        trades = load_model_trades(model_id)
        if trades:
            analysis = analyze_model_thinking(model_id, trades)
            all_analysis[model_id] = analysis
    
    # Generate report
    generate_markdown_report(all_analysis)
    
    print("\n" + "=" * 60)
    print("COT ANALYSIS COMPLETE!")
    print("=" * 60)
    print(f"Report: {OUTPUT_DIR}/{OUTPUT_REPORT}")


if __name__ == "__main__":
    main()

